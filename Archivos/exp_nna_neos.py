# -*- coding: utf-8 -*-
"""Exp_NNA_NEOS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fJ5JR0PYsUXLcsUuO9cv456a4S7hTAi8

Experimento 1: programa que utiliza los datos de la enfermedad Neospora. Se realiza el proceso de análisis exploratorio, selección de características, ajuste de la red neuronal, entrenamiento, validación y al final se guarda el modelo.

### Cargar datos
"""

# Las librerias a utilizar
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

!pip install joblib

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import joblib

# Montar el drive para consumir datos
from google.colab import drive
drive.mount('/content/drive')

"""Análisis exploratorio de datos."""

# Cargar Datos
Data = pd.read_excel('/content/drive/My Drive/DOCTORADO/Academica/THESIS/Experimentos/integrar/Neos.xlsx')

#mostrar los 5 primeros datos (generalidad de las observaciones)
Data.head()

#Logitud final
len(Data)

train_labels = Data['NEOSPORA']

columns_to_extract = ['EDAD','RAZA','ABORTO','REPETICION','NOCARGA','DISTOCIAS','TERNEROS_DEBILES','MUERTE','MONTA_NATURAL','INSEMINACION','BEBEDEROS','PASTOS','CORRAL','TIPO_ORDEÑO']

train_features = Data[columns_to_extract]

"""## Exploración de datos"""

Data['EDAD'].unique()

Data['RAZA'].unique()

"""Codificación ONE-HOT-ENCODING - Pandas
2 años 100
3 años 010
4 años 001
"""

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()

train_features = pd.concat([train_features, pd.get_dummies(train_features['EDAD'],prefix='EDAD')], axis= 1)
train_features.drop(['EDAD'], axis=1, inplace=True)
train_features = pd.concat([train_features, pd.get_dummies(train_features['RAZA'],prefix='RAZA')], axis= 1)
train_features.drop(['RAZA'], axis=1, inplace=True)

train_features

train_labels

"""Datos de laboratorio. Son procesados por el laboratorio y entregan el archivo siguiendo el estándar del Kit."""

dataL = pd.read_excel('/content/drive/My Drive/DOCTORADO/Academica/THESIS/Experimentos/integrar/LabNeos.xlsx')

dataL.shape

dataL.columns

dataL.head()

"""Transofmración de características a un rango dado. Teniendo en cuenta que la red solo entiende datos en rangos."""

from sklearn import preprocessing

min_max_scaler = preprocessing.StandardScaler()
x_scaled = min_max_scaler.fit_transform(dataL)
df = pd.DataFrame(x_scaled)

df

df.columns=['NEOS','BLANCO','ControlP','ControlN','RATIO']

"""Concatenar los dos archivos"""

newData = pd.concat([train_features, df], axis=1,)
newData

"""Hasta este punto se leyó el conjunto de datos, se realizó el proceso de normalización de las variables númericas y se trasnformaron las variables categóricas.

## 3) Utilizando redes neuronales tipo feedforward cree un modelo para predecir la presencia de las enfermedades.

#Definicion del perfil de datos y variable objetiva --> (lo que vamos a predecir!)
"""

# Conjunto de entrenamiento y validacion
X = newData # Matriz de caracteristicas
y = train_labels

from sklearn.model_selection import train_test_split

# definiendo el porcentaje de particion en 70% entrenar y 30% para test validacion
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=101)

from keras.models import Sequential # Arma la arquitectura bajo el modelo secuencial *(Nos permite utilizar la funcion (Add))
from keras.layers import Dense, Dropout # Lo que nos permite llamr a las funciones de activacion, tama;o de entrada o inputs y dispnemos de las neuronas o nodos
from tensorflow.keras.optimizers import SGD, Adam, Adadelta # Tenemos a los optimizadores

"""Se propone una Red Neuronal, propia, del cual despues de varios intentos se llega a esta configuración, donde se dispone de 20, 32, 16, 8, 4 y 1 neuronas como aouput, dada la naturaleza de la salida, al predecir un valor binario. De esta manera, se realiza esta arquitectura decreciente en nodos, para afectar el comportamiento y estabilidad a la hora de evidenciar en el gráfico de historia las posibles convergencias.

Nota: Para todos los experimentos se implementan callback, con paciencia de 10 realizaciones y con un minimo delta que evalua lo sensible del modelo de 0.001.
"""

model = Sequential()
model.add(Dense(24,input_shape=(24,),activation='tanh'))
model.add(Dense(32,activation='relu'))
model.add(Dense(16,activation='relu'))
model.add(Dense(8,activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])

model.summary()

from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
import datetime

"""# Red Neuronal Feedforward"""

path = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback1 = TensorBoard(log_dir=path,histogram_freq=1)

ES = EarlyStopping(monitor='val_loss',min_delta=0.0001,patience=10,verbose=1,restore_best_weights=True)
MP = ModelCheckpoint(filepath="/content/drive/My Drive/DOCTORADO/Academica/THESIS/Experimentos/integrar/ModelosNEOS/pesos.{epoch:02d}.hdf5",save_best_only=True)

history = model.fit(X_train,y_train,validation_data=(X_test,y_test), validation_split=0.2,epochs=100,callbacks=[ES,MP,tensorboard_callback1])

test_loss, test_acc = model.evaluate(X, y, verbose = 2)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Curva de la función de pérdida')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend(['Train','test'], loc='upper left')
plt.show()

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Curva de exactitud')
plt.ylabel('accuracy')
plt.xlabel('Epoch')
plt.legend(['Train','test'], loc='upper left')
plt.show()

"""Predicciones con los datos de Test, que se toman de los datos originales y en la división quedaron 300."""

predictionLR = model.predict(X_test)

out = predictionLR.round().astype(int)

"""Métricas adicionales"""

from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
from sklearn.metrics import precision_score, make_scorer, accuracy_score

print('MAE',mean_absolute_error(y_test,predictionLR)) #Media del error absoluto
print('MSE',mean_squared_error(y_test,predictionLR)) #Media del error cuadratico
print('RMSE',np.sqrt(mean_squared_error(y_test,predictionLR))) #Raiz cuadrada de la media del error cuadratico
print("Certeza del modelo","{0:.2f}%".format(r2_score(y_test,predictionLR)*100))

model.fit(X_train, y_train)

from numpy.lib.function_base import average
pre_scorer = make_scorer(score_func = precision_score, pos_label = 1, greater_is_better=True, average='micro')

"""Validación del modelo con datos que no conoce el modelo. Se subre el archivo de 460 datos (2021)."""

Text_Val = pd.read_excel('/content/drive/My Drive/DOCTORADO/Academica/THESIS/Experimentos/integrar/Datos21.xlsx')

Text_Val

val_Lab= pd.read_excel('/content/drive/My Drive/DOCTORADO/Academica/THESIS/Experimentos/integrar/val_Lab.xlsx')

min_max_scaler = preprocessing.StandardScaler()
x_scaled = min_max_scaler.fit_transform(val_Lab)
df = pd.DataFrame(x_scaled)

Text_Val = pd.concat([Text_Val, pd.get_dummies(Text_Val['EDAD'],prefix='EDAD')], axis= 1)
Text_Val.drop(['EDAD'], axis=1, inplace=True)
Text_Val = pd.concat([Text_Val, pd.get_dummies(Text_Val['RAZA'],prefix='RAZA')], axis= 1)
Text_Val.drop(['RAZA'], axis=1, inplace=True)

Text_Val

nData = pd.concat([Text_Val, df], axis=1,)

nData

pred = model.predict(nData)

pred

out1= pred.round().astype(int)
out1

out1.flatten().tolist()

df = pd.DataFrame(data=out1)
df.to_excel('/content/drive/My Drive/DOCTORADO/Academica/THESIS/Experimentos/integrar/res.xlsx')

"""# Guardar el modelo"""

# Guardar modelo
joblib.dump(model,'mol_diseases.pkl')



